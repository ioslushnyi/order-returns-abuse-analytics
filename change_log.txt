## Just a reference file with changes to help me track the decisions and changes

16/07/2025 - day 1 - added approx repo structure, started working on fake data generation
18/07/2025 - day 2 - ran spark job locally, created parquete files. next step is to run job on data proc
19/07/2025 - day 3 - set up dbt project, added basic models




# Set project ID and region
export PROJECT_ID="order-returns-abuse-detection"
export REGION="US"
export BUCKET_NAME="order-returns-data"
export BQ_DATASET="order_returns"

# Set project
gcloud config set project $PROJECT_ID

# Enable required APIs
gcloud services enable \
  bigquery.googleapis.com \
  composer.googleapis.com \
  storage.googleapis.com \
  dataproc.googleapis.com \
  cloudbuild.googleapis.com \
  cloudfunctions.googleapis.com

# Create GCS bucket for data
gsutil mb -l $REGION gs://$BUCKET_NAME/


# Create BigQuery dataset
bq --location=$REGION mk --dataset $PROJECT_ID:$BQ_DATASET

steps to setup the project:

# Clone repo
git clone https://github.com/ioslushnyi/order-returns-abuse-detection.git
cd order-returns-abuse-detection

#setup virtual environment:
python -m venv .venv
.\.venv\Scripts\activate
#install dependenciesL
pip install -r requirements.txt

# Optionaly (if ./data/*.csv is not in already in the repo) build data:
# cd datagenerator
# python generate_all_data.py

# Optional: create folders for raw
gsutil -m cp -r ./data/*.csv gs://$BUCKET_NAME/raw/

# Optional: run spark job locally
cd spark-jobs
python clean_returns.py
cd ..

# Optional: create folders for clean
gsutil -m cp -r ./data/clean_returns/ gs://$BUCKET_NAME/clean/

# Create BigQuery table (schema inferred from Parquet)
# version if partitioning was not used when running spark cleaning job
bq load --source_format=PARQUET 
  --autodetect 
  order_returns.cleaned_returns 
  gs://BUCKET_NAME/clean/clean_returns/*.parquet
# test
bq query --nouse_legacy_sql   "SELECT return_reason, COUNT(*) as cnt FROM order_returns.cleaned_returns GROUP BY return_reason"

# --- OPTIONAL HELP FOR REFERENCE ---
# Delete the old parquete files on GCS
gsutil -m rm -r gs://order-returns-data/clean/clean_returns/
# Create & upload again
gsutil -m cp -r ./data/clean_returns/ gs://$BUCKET_NAME/clean/
# Delete the old/broken table
bq rm -t order_returns.cleaned_returns
# version if partitioning WAS USED when running spark cleaning job
bq load \
  --source_format=PARQUET \
  --autodetect \
  --hive_partitioning_mode=AUTO \
  --hive_partitioning_source_uri_prefix=gs://order-returns-data/clean/clean_returns/ \
  order_returns.cleaned_returns \
  'gs://order-returns-data/clean/clean_returns/*.parquet'
# --- OPTIONAL HELP FOR REFERENCE ---


# --- DBT ---
# After sample data is generated, clean via spark job, uploaded to GCS as parquet and then to BQ, setup dbt in the project:
# init dbt
cd dbt
dbt init order_returns_analytics

# Authenticate with BigQuery (use Oauth or service account)
dbt debug